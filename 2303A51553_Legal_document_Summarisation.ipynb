{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoeSupJ8QivPEaxu6ojIHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51553/Natural-language-process/blob/main/2303A51553_Legal_document_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay-sf9PRx6rE",
        "outputId": "8150cd93-72c0-4ab8-9086-84cd9d3f5d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow nltk pandas numpy sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlPv_Z9NyT1p",
        "outputId": "0a1a6992-9a4d-461b-c557-a3e83b751f7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "df = pd.read_csv(\"/content/Legal_Summarisation_100_Final.csv\")\n",
        "\n",
        "df[\"clean_doc\"] = df[\"document\"].apply(preprocess_text)\n",
        "df[\"clean_sum\"] = df[\"summary\"].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qCyCaQKynVb",
        "outputId": "48352a9b-23f1-4515-b73f-b209bf811b75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = \"[BOS]\"\n",
        "EOS = \"[EOS]\"\n",
        "\n",
        "df[\"clean_sum\"] = df[\"clean_sum\"].apply(lambda x: f\"{BOS} {x} {EOS}\")\n"
      ],
      "metadata": {
        "id": "tIl8dmYLy5En"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 25000\n",
        "max_enc_len = 256\n",
        "max_dec_len = 64\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=max_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_dec_len,\n",
        "    standardize=None\n",
        ")\n",
        "\n",
        "# Adapt vocabulary\n",
        "vectorizer.adapt(df[\"clean_doc\"].tolist() + df[\"clean_sum\"].tolist())\n",
        "\n",
        "# Get vocab\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Token IDs\n",
        "BOS_ID = vocab.index(BOS)\n",
        "EOS_ID = vocab.index(EOS)\n",
        "PAD_ID = 0\n"
      ],
      "metadata": {
        "id": "iJME-un1zBL9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = vectorizer(df[\"clean_doc\"].tolist()).numpy()\n",
        "dec = vectorizer(df[\"clean_sum\"].tolist()).numpy()\n",
        "\n",
        "# Decoder input starts with BOS\n",
        "dec_in = np.concatenate([np.full((dec.shape[0],1), BOS_ID), dec[:,:-1]], axis=1)\n",
        "\n",
        "# Decoder target is full summary\n",
        "dec_out = dec\n"
      ],
      "metadata": {
        "id": "dfoaSEKpzFMR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.token_emb=layers.Embedding(vocab_size, embed_dim, mask_zero=True)\n",
        "        self.pos_emb=layers.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        positions=tf.range(start=0, limit=tf.shape(x)[1])\n",
        "        pos_embeddings=self.pos_emb(positions)\n",
        "        tok_embeddings=self.token_emb(x)\n",
        "        return tok_embeddings+pos_embeddings\n"
      ],
      "metadata": {
        "id": "yxYNVBKEzE8K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encoder_layer(embed_dim, num_heads, ff_dim):\n",
        "    inp=layers.Input(shape=(None, embed_dim))\n",
        "    attn=layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inp, inp)\n",
        "    out1=layers.LayerNormalization()(inp + attn)\n",
        "\n",
        "    ffn=layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
        "    ffn=layers.Dense(embed_dim)(ffn)\n",
        "    out2=layers.LayerNormalization()(out1 + ffn)\n",
        "\n",
        "    return keras.Model(inp, out2)\n"
      ],
      "metadata": {
        "id": "oSe5dc5uzOuB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_decoder_layer(embed_dim, num_heads, ff_dim):\n",
        "    dec_in=layers.Input(shape=(None, embed_dim))\n",
        "    enc_out=layers.Input(shape=(None, embed_dim))\n",
        "\n",
        "    # Masked self-attention\n",
        "    attn1=layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim, use_causal_mask=True\n",
        "    )(dec_in, dec_in)\n",
        "    out1=layers.LayerNormalization()(dec_in + attn1)\n",
        "\n",
        "    # Cross-attention\n",
        "    attn2=layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )(out1, enc_out)\n",
        "    out2=layers.LayerNormalization()(out1 + attn2)\n",
        "\n",
        "    # Feed-forward\n",
        "    ffn=layers.Dense(ff_dim, activation=\"relu\")(out2)\n",
        "    ffn=layers.Dense(embed_dim)(ffn)\n",
        "    out3=layers.LayerNormalization()(out2 + ffn)\n",
        "\n",
        "    return keras.Model([dec_in, enc_out],out3)\n"
      ],
      "metadata": {
        "id": "Ghm8KooYzSNJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim=256\n",
        "ff_dim=512\n",
        "num_heads=8\n",
        "num_layers=4\n",
        "\n",
        "def bert_decoder_layer(embed_dim, num_heads, ff_dim):\n",
        "    dec_in=layers.Input(shape=(None, embed_dim))\n",
        "    enc_out=layers.Input(shape=(None, embed_dim))\n",
        "\n",
        "    # Instantiate MultiHeadAttention layers\n",
        "    mha_self_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    mha_cross_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "    # Masked self-attention: pass use_causal_mask=True to the call method\n",
        "    attn1=mha_self_attn(query=dec_in, value=dec_in, use_causal_mask=True)\n",
        "    out1=layers.LayerNormalization()(dec_in + attn1)\n",
        "\n",
        "    # Cross-attention\n",
        "    attn2=mha_cross_attn(query=out1, value=enc_out, key=enc_out)\n",
        "    out2=layers.LayerNormalization()(out1 + attn2)\n",
        "\n",
        "    # Feed-forward\n",
        "    ffn=layers.Dense(ff_dim, activation=\"relu\")(out2)\n",
        "    ffn=layers.Dense(embed_dim)(ffn)\n",
        "    out3=layers.LayerNormalization()(out2 + ffn)\n",
        "\n",
        "    return keras.Model([dec_in, enc_out],out3)\n",
        "\n",
        "# Encoder\n",
        "enc_input = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "enc_emb = PositionalEmbedding(vocab_size, embed_dim, max_enc_len)(enc_input)\n",
        "encoder = enc_emb\n",
        "for _ in range(num_layers):\n",
        "    encoder = bert_encoder_layer(embed_dim, num_heads, ff_dim)(encoder)\n",
        "\n",
        "# Decoder\n",
        "dec_input=keras.Input(shape=(None,),dtype=\"int32\")\n",
        "dec_emb=PositionalEmbedding(vocab_size,embed_dim, max_dec_len)(dec_input)\n",
        "decoder=dec_emb\n",
        "for _ in range(num_layers):\n",
        "    decoder=bert_decoder_layer(embed_dim, num_heads,ff_dim)([decoder,encoder])\n",
        "\n",
        "# Output\n",
        "final_output=layers.Dense(vocab_size)(decoder)\n",
        "\n",
        "model=keras.Model([enc_input,dec_input],final_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "07X8JpzkzW1l",
        "outputId": "a5d77468-fa6b-4c75-8d3a-15238a50be9d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_37\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_37\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_54      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m89,088\u001b[0m │ input_layer_54[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_29       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,367,488\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_30       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,367,488\u001b[0m │ functional_29[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_59      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_31       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,367,488\u001b[0m │ functional_30[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m39,936\u001b[0m │ input_layer_59[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_32       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,367,488\u001b[0m │ functional_31[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_33       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m4,471,552\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_32[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_34       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m4,471,552\u001b[0m │ functional_33[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_32[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_35       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m4,471,552\u001b[0m │ functional_34[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_32[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_36       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m4,471,552\u001b[0m │ functional_35[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_32[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m)  │     \u001b[38;5;34m23,644\u001b[0m │ functional_36[\u001b[38;5;34m0\u001b[0m]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_54      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">89,088</span> │ input_layer_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_29       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,367,488</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_30       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,367,488</span> │ functional_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_59      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_31       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,367,488</span> │ functional_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,936</span> │ input_layer_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_32       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,367,488</span> │ functional_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_33       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,471,552</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_34       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,471,552</span> │ functional_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_35       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,471,552</span> │ functional_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_36       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,471,552</span> │ functional_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,644</span> │ functional_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,508,828\u001b[0m (104.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,508,828</span> (104.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,508,828\u001b[0m (104.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,508,828</span> (104.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss=loss_fn)\n",
        "\n",
        "model.fit([enc, dec_in],dec_out,batch_size=4,epochs=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCCVa264zibC",
        "outputId": "aea93045-9fba-424d-f09d-894c2dcb606d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 2.6583\n",
            "Epoch 2/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - loss: 0.8239\n",
            "Epoch 3/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 3s/step - loss: 0.8138\n",
            "Epoch 4/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - loss: 0.8160\n",
            "Epoch 5/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - loss: 0.8068\n",
            "Epoch 6/6\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - loss: 0.8049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79b48fe70ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_word={i:w for i,w in enumerate(vocab)}\n",
        "\n",
        "def generate(text, max_len=60):\n",
        "    e=vectorizer([text]).numpy()\n",
        "    d=np.zeros((1, max_len), dtype=int)\n",
        "    d[0,0]=BOS_ID\n",
        "\n",
        "    for i in range(1, max_len):\n",
        "        preds=model.predict([e, d[:,:i]],verbose=0)\n",
        "        next_id=np.argmax(preds[0, i-1])\n",
        "\n",
        "        if next_id==EOS_ID:\n",
        "            break\n",
        "\n",
        "        d[0,i]=next_id\n",
        "\n",
        "    tokens=[id_to_word[t] for t in d[0] if t not in [PAD_ID, BOS_ID, EOS_ID]]\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "DxTK2FYmzz4-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "preds=[]\n",
        "refs=[]\n",
        "\n",
        "for i in range(30):\n",
        "    p=generate(df[\"clean_doc\"].iloc[i])\n",
        "    preds.append(p)\n",
        "    refs.append(df[\"clean_sum\"].iloc[i])\n",
        "\n",
        "bleu=sacrebleu.corpus_bleu(preds, [refs])\n",
        "print(\"BLEU Score:\", bleu.score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XA4RqpV0Bmn",
        "outputId": "9162743e-9ba1-4430-a523-567b72dc8dfc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.046290446176410774\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWnyj7AZkLSBiqYma4pKBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51553/Natural-language-process/blob/main/legal_document_summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMxbAFyt01MP"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow nltk numpy pandas sacrebleu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "df = pd.read_csv(\"/content/Legal_Summarisation_100_Final.csv\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text=str(text).lower()\n",
        "    text=text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text=re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens=word_tokenize(text)\n",
        "    tokens=[w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"clean_doc\"]=df[\"document\"].apply(preprocess_text)\n",
        "df[\"clean_sum\"]=df[\"summary\"].apply(preprocess_text)\n",
        "df[\"clean_sum\"] = \"bos \"+df[\"clean_sum\"]+\" eos\""
      ],
      "metadata": {
        "id": "vsPhkQvy2xV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab=6000\n",
        "max_enc_len=256\n",
        "max_dec_len=64\n",
        "\n",
        "vectorizer=layers.TextVectorization(\n",
        "    max_tokens=max_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_enc_len,\n",
        "    standardize=None\n",
        ")\n",
        "\n",
        "vectorizer.adapt(df[\"clean_doc\"].tolist() + df[\"clean_sum\"].tolist())\n",
        "\n",
        "vocab=vectorizer.get_vocabulary()\n",
        "vocab_size=len(vocab)\n"
      ],
      "metadata": {
        "id": "KquaJ95p3dfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc=vectorizer(df[\"clean_doc\"].tolist()).numpy()\n",
        "dec=vectorizer(df[\"clean_sum\"].tolist()).numpy()\n",
        "\n",
        "# teacher forcing shift\n",
        "dec_in = np.concatenate([np.zeros((dec.shape[0],1)), dec[:,:-1]], axis=1)\n",
        "dec_out = dec\n"
      ],
      "metadata": {
        "id": "ktsNzhI5n6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.token_emb=layers.Embedding(vocab_size, embed_dim, mask_zero=True)\n",
        "        self.pos_emb=layers.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        max_len=tf.shape(x)[1]\n",
        "        positions=tf.range(start=0, limit=max_len)\n",
        "        pos_embeddings=self.pos_emb(positions)\n",
        "        tok_embeddings=self.token_emb(x)\n",
        "        return tok_embeddings + pos_embeddings\n"
      ],
      "metadata": {
        "id": "uTbLxloD4_Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encoder_layer(embed_dim, num_heads, ff_dim):\n",
        "    inputs=layers.Input(shape=(None, embed_dim))\n",
        "    attn=layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
        "    out1=layers.LayerNormalization()(inputs + attn)\n",
        "\n",
        "    ffn=layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
        "    ffn=layers.Dense(embed_dim)(ffn)\n",
        "    out2=layers.LayerNormalization()(out1 + ffn)\n",
        "\n",
        "    return keras.Model(inputs, out2)\n"
      ],
      "metadata": {
        "id": "-vLPlh756_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_decoder_layer(embed_dim, num_heads, ff_dim):\n",
        "    dec_in=layers.Input(shape=(None, embed_dim))\n",
        "    enc_out=layers.Input(shape=(None, embed_dim))\n",
        "\n",
        "    attn1=layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(dec_in, dec_in, use_causal_mask=True)\n",
        "    out1=layers.LayerNormalization()(dec_in + attn1)\n",
        "\n",
        "    attn2=layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(out1, enc_out)\n",
        "    out2=layers.LayerNormalization()(out1 + attn2)\n",
        "\n",
        "    ffn=layers.Dense(ff_dim, activation=\"relu\")(out2)\n",
        "    ffn=layers.Dense(embed_dim)(ffn)\n",
        "    out3=layers.LayerNormalization()(out2 + ffn)\n",
        "\n",
        "    return keras.Model([dec_in, enc_out], out3)\n"
      ],
      "metadata": {
        "id": "gOA23mUn7ChB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim=256\n",
        "ff_dim=512\n",
        "num_heads=8\n",
        "num_layers=4\n",
        "\n",
        "# Encoder\n",
        "enc_input=keras.Input(shape=(None,), dtype=\"int32\")\n",
        "enc_emb=PositionalEmbedding(vocab_size, embed_dim, max_enc_len)(enc_input)\n",
        "encoder=enc_emb\n",
        "for _ in range(num_layers):\n",
        "    encoder=bert_encoder_layer(embed_dim, num_heads, ff_dim)(encoder)\n",
        "\n",
        "# Decoder\n",
        "dec_input=keras.Input(shape=(None,), dtype=\"int32\")\n",
        "dec_emb=PositionalEmbedding(vocab_size, embed_dim, max_dec_len)(dec_input)\n",
        "decoder=dec_emb\n",
        "for _ in range(num_layers):\n",
        "    decoder=bert_decoder_layer(embed_dim, num_heads, ff_dim)([decoder, encoder])\n",
        "\n",
        "# Output to vocab\n",
        "final_output=layers.Dense(vocab_size)(decoder)\n",
        "\n",
        "model=keras.Model([enc_input, dec_input], final_output)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "0QhQoQE57Hly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc=vectorizer(df[\"clean_doc\"].tolist()).numpy()\n",
        "dec=vectorizer(df[\"clean_sum\"].tolist()).numpy()[:, :max_dec_len]\n",
        "\n",
        "dec_in =np.concatenate([np.zeros((dec.shape[0],1)), dec[:,:-1]], axis=1)\n",
        "dec_out=dec"
      ],
      "metadata": {
        "id": "-H7FKw7N7SNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")\n",
        "\n",
        "model.fit([enc, dec_in], dec_out, epochs=5, batch_size=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCIZ0iE-7ZuK",
        "outputId": "135c2c86-37d2-442d-d983-db2f11cbf25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 5s/step - loss: 2.8572\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 4s/step - loss: 0.8334\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4s/step - loss: 0.8199\n",
            "Epoch 4/5\n",
            "\u001b[1m 3/25\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 4s/step - loss: 0.8187"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating summary"
      ],
      "metadata": {
        "id": "5rUZkAHd7gg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_word={i:w for i,w in enumerate(vocab)}\n",
        "\n",
        "def generate(text):\n",
        "    e=vectorizer([text]).numpy()\n",
        "    d=np.zeros((1, max_dec_len), dtype=int)\n",
        "\n",
        "    for i in range(1, max_dec_len):\n",
        "        preds=model.predict([e, d[:,:i]], verbose=0)\n",
        "        next_id=np.argmax(preds[0, i-1])\n",
        "        d[0, i]=next_id\n",
        "        if next_id==0:\n",
        "            break\n",
        "\n",
        "    words=[id_to_word.get(i,\"\") for i in d[0] if i!=0]\n",
        "    return \" \".join(words)\n",
        "\n",
        "text=df[\"clean_doc\"].iloc[0] # Changed from .iloc[0:10] to .iloc[0]\n",
        "summary=generate(text)\n",
        "print(\"Generated Summary:\\n\", text)"
      ],
      "metadata": {
        "id": "OyE_xKIL7dmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "preds=[]\n",
        "refs=[]\n",
        "\n",
        "for i in range(30):  # evaluate 30 examples\n",
        "    pred=generate(df[\"clean_doc\"].iloc[i])\n",
        "    preds.append(pred)\n",
        "    refs.append(df[\"clean_sum\"].iloc[i].replace(\"bos \",\"\").replace(\" eos\",\"\"))\n",
        "\n",
        "bleu=sacrebleu.corpus_bleu(preds,what is bos eos [refs],tokenize=\"flores200\")\n",
        "\n",
        "print(\"BLEU Score:\", \"{:.10f}\".format(bleu.score))\n",
        "\n"
      ],
      "metadata": {
        "id": "xUrAhQxC9b84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51553/Natural-language-process/blob/main/legal_document_summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMxbAFyt01MP",
        "outputId": "bdfa94ea-5320-4ed1-8e35-ea442dacbe49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow nltk numpy pandas sacrebleu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZSwDOjRufME",
        "outputId": "e1a13b1e-bf61-4fb6-bb6d-367c98d9086b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples after cleaning:\n",
            "                                                                                                                                                                                                                                            cleaned_document                                          cleaned_summary\n",
            "                        matter concerns constitutional validity law petitioner argues violation rights articles 14 21 respondent defends validity action public interest high court decision challenged supreme court hearing sides court delivered judgment      supreme court rules constitutional validity statute\n",
            "matter concerns public interest litigation environmental protection petitioner argues violation rights articles 14 21 respondent defends validity action public interest high court decision challenged supreme court hearing sides court delivered judgment court directs stronger measures environmental protection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df = pd.read_csv(\"/content/Legal_Summarisation_100_Final (1).csv\")\n",
        "df = df.dropna(subset=['document','summary']).reset_index(drop=True)\n",
        "df['cleaned_document'] = df['document'].apply(preprocess_text)\n",
        "df['cleaned_summary']  = df['summary'].apply(preprocess_text)\n",
        "print(\"Examples after cleaning:\")\n",
        "print(df[['cleaned_document','cleaned_summary']].head(2).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Fqs6dRuodV",
        "outputId": "d1058cb4-b19c-4c0e-e940-7a4797e2053c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 90\n"
          ]
        }
      ],
      "source": [
        "max_vocab_size=6000\n",
        "max_enc_len=256\n",
        "max_dec_len=64\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=max_vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_enc_len,\n",
        "    standardize=None\n",
        ")\n",
        "\n",
        "vectorizer.adapt(np.concatenate([df['cleaned_document'].values, df['cleaned_summary'].values]))\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ktsNzhI5n6Y3"
      },
      "outputs": [],
      "source": [
        "pad_id = 0\n",
        "def encode_input(texts):\n",
        "    return vectorizer(tf.constant(texts)).numpy()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_decoder_sequences(texts):\n",
        "    token_ids = vectorizer(tf.constant(texts)).numpy()\n",
        "    return token_ids"
      ],
      "metadata": {
        "id": "2rmh5KuaUQgg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, maxlen):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, x):\n",
        "        # x: (batch, seq)\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions  # (batch, seq, embed_dim)\n",
        "\n",
        "def transformer_encoder_layer(embed_dim, ff_dim, num_heads, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(None, embed_dim))\n",
        "    # Self-attention\n",
        "    attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)(inputs, inputs)\n",
        "    attn = layers.Dropout(dropout_rate)(attn)\n",
        "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn)\n",
        "    # Feed-forward\n",
        "    ff = layers.Dense(ff_dim, activation='relu')(out1)\n",
        "    ff = layers.Dense(embed_dim)(ff)\n",
        "    ff = layers.Dropout(dropout_rate)(ff)\n",
        "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ff)\n",
        "    return keras.Model(inputs=inputs, outputs=out2) # Removed name argument\n",
        "\n",
        "def transformer_decoder_layer(embed_dim, ff_dim, num_heads, dropout_rate=0.1):\n",
        "    dec_inputs = layers.Input(shape=(None, embed_dim))      # decoder embeddings\n",
        "    enc_outputs = layers.Input(shape=(None, embed_dim))     # encoder outputs\n",
        "    # Self-attention with causal mask handled in call\n",
        "    attn1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)(dec_inputs, dec_inputs, use_causal_mask=True)\n",
        "    attn1 = layers.Dropout(dropout_rate)(attn1)\n",
        "    out1 = layers.LayerNormalization(epsilon=1e-6)(dec_inputs + attn1)\n",
        "    # Cross-attention\n",
        "    attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)(out1, enc_outputs)\n",
        "    attn2 = layers.Dropout(dropout_rate)(attn2)\n",
        "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
        "    # Feed-forward\n",
        "    ff = layers.Dense(ff_dim, activation='relu')(out2)\n",
        "    ff = layers.Dense(embed_dim)(ff)\n",
        "    ff = layers.Dropout(dropout_rate)(ff)\n",
        "    out3 = layers.LayerNormalization(epsilon=1e-6)(out2 + ff)\n",
        "    return keras.Model([dec_inputs, enc_outputs], out3) # Removed name argument"
      ],
      "metadata": {
        "id": "umcM6khiUqQl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "\n",
        "# Encoder\n",
        "enc_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "enc_emb = PositionalEmbedding(vocab_size, embed_dim, max_enc_len)(enc_inputs)\n",
        "encoder_output = enc_emb\n",
        "for _ in range(num_layers):\n",
        "    encoder_output = transformer_encoder_layer(embed_dim, ff_dim, num_heads, dropout_rate)(encoder_output)\n",
        "\n",
        "# Decoder\n",
        "dec_inputs = keras.Input(shape=(max_dec_len,), dtype=\"int64\") # Fixed: Explicitly set max_dec_len\n",
        "dec_emb = PositionalEmbedding(vocab_size, embed_dim, max_dec_len)(dec_inputs)\n",
        "decoder_output = dec_emb\n",
        "for _ in range(num_layers):\n",
        "    decoder_output = transformer_decoder_layer(embed_dim, ff_dim, num_heads, dropout_rate)([decoder_output, encoder_output])\n",
        "\n",
        "logits = layers.Dense(vocab_size)(decoder_output)  # (batch, seq_dec, vocab_size)\n",
        "\n",
        "model = keras.Model([enc_inputs, dec_inputs], logits, name=\"bert_like_seq2seq\")\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# Loss, optimizer, compile\n",
        "# ---------------------------\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    # y_true: (batch, seq)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, pad_id), dtype=loss.dtype)\n",
        "    loss = loss * mask\n",
        "    return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-6)\n",
        "\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss=masked_loss, metrics=metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "3kMLJgFOU8sm",
        "outputId": "b2c4a477-968f-477b-e7ee-c912b72b27a6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"bert_like_seq2seq\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_like_seq2seq\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_40      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m88,576\u001b[0m │ input_layer_40[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_16       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m527,104\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_17       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m527,104\u001b[0m │ functional_16[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_45      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_18       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m527,104\u001b[0m │ functional_17[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m39,424\u001b[0m │ input_layer_45[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_19       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m527,104\u001b[0m │ functional_18[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_20       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m790,784\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_19[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_21       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m790,784\u001b[0m │ functional_20[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_19[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_22       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m790,784\u001b[0m │ functional_21[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_19[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_23       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m790,784\u001b[0m │ functional_22[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_19[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m90\u001b[0m)    │     \u001b[38;5;34m23,130\u001b[0m │ functional_23[\u001b[38;5;34m0\u001b[0m]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_40      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">88,576</span> │ input_layer_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_16       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_17       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ functional_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_45      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_18       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ functional_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,424</span> │ input_layer_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_19       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ functional_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_20       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">790,784</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_21       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">790,784</span> │ functional_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_22       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">790,784</span> │ functional_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_23       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">790,784</span> │ functional_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,130</span> │ functional_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,422,682\u001b[0m (20.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,422,682</span> (20.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,422,682\u001b[0m (20.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,422,682</span> (20.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-vLPlh756_UL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2cb166-93ea-4ee1-c1d7-07cb3d15ef66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 2s/step - loss: 4.4007 - sparse_categorical_accuracy: 0.0100 - val_loss: 2.7807 - val_sparse_categorical_accuracy: 0.0302\n",
            "Epoch 2/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 1.9209 - sparse_categorical_accuracy: 0.0530 - val_loss: 1.4551 - val_sparse_categorical_accuracy: 0.0667\n",
            "Epoch 3/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 0.9775 - sparse_categorical_accuracy: 0.0756 - val_loss: 0.7952 - val_sparse_categorical_accuracy: 0.0760\n",
            "Epoch 4/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 0.6268 - sparse_categorical_accuracy: 0.0794 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.0750\n",
            "Epoch 5/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.5470 - sparse_categorical_accuracy: 0.0760 - val_loss: 0.5297 - val_sparse_categorical_accuracy: 0.0760\n",
            "Epoch 6/6\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.4942 - sparse_categorical_accuracy: 0.0791 - val_loss: 0.4919 - val_sparse_categorical_accuracy: 0.0760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7beb426815e0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "epochs = 6\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTbLxloD4_Ui"
      },
      "source": [
        "id_to_word = {i:w for i,w in enumerate(vocab)}\n",
        "word_to_id = {w:i for i,w in enumerate(vocab)}\n",
        "\n",
        "def greedy_decode(input_text, max_dec_steps=60):\n",
        "    enc_seq = encode_input([input_text])  # shape (1, max_enc_len)\n",
        "    # start token is pad_id (0) as used earlier\n",
        "    dec_seq = np.full((1, max_dec_len), pad_id, dtype=np.int32)\n",
        "    # initial decoder input: start token at position 0\n",
        "    dec_seq[0,0] = pad_id\n",
        "    for i in range(1, max_dec_steps):\n",
        "        # prepare truncated input for speed: feed upto i positions\n",
        "        dec_input_slice = dec_seq[:, :i]\n",
        "        preds = model.predict([enc_seq, dec_input_slice], verbose=0)  # (1, i, vocab)\n",
        "        next_token_logits = preds[0, -1, :]  # last timestep logits\n",
        "        next_id = int(np.argmax(next_token_logits))\n",
        "        dec_seq[0, i] = next_id\n",
        "        # stop if we predict padding (used as pseudo-SEP) or EOS equivalent; no explicit SEP here\n",
        "        if next_id == pad_id:\n",
        "            break\n",
        "    # convert dec_seq tokens back to words, skip pads at beginning\n",
        "    tokens = []\n",
        "    for id in dec_seq[0]:\n",
        "        if id == pad_id:\n",
        "            continue\n",
        "        tokens.append(id_to_word.get(int(id), \"\"))\n",
        "    return \" \".join([t for t in tokens if t])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refs=[]\n",
        "preds=[]\n",
        "num_eval=min(100, len(val_df))\n",
        "for idx in range(num_eval):\n",
        "    src=val_df['cleaned_document'].iloc[idx]\n",
        "    tgt=val_df['cleaned_summary'].iloc[idx]\n",
        "    pred_text=greedy_decode(src, max_dec_steps=60)\n",
        "\n",
        "    refs.append([tgt.split()])\n",
        "    preds.append(pred_text.split())\n",
        "\n",
        "smooth=SmoothingFunction().method1\n",
        "bleu_score=corpus_bleu(refs, preds,smoothing_function=smooth)\n",
        "print(\"BLEU (corpus) on validation subset:\",bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT72rQ8zYUcJ",
        "outputId": "8f370e9f-a802-41b7-85de-e4ff799de420"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU (corpus) on validation subset: 0.017894467682407687\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw6EUKFHvDi995qO0pFjlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}